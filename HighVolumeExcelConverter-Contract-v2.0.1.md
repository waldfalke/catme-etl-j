# Контракт модуля: `HighVolumeExcelConverter`

## Метаданные

*   **Версия**: 2.0.1 (уточненная)
*   **Статус**: Проектируется
*   **Дата**: 2025-05-12 (Предложение), 2025-MM-DD (Уточнение)
*   **Ответственный**: Главный архитектор CATMEPIM
*   **Назначение ревизии**: Уточнение деталей на основе обратной связи: установка ZipSecureFile, порядок строк, именование CSV, выбор листа, обработка типов данных, отсутствие восстановления, требования к RAM.

---

## 1. Назначение и место в системе

**Назначение:** Обеспечить потоковую, высокопроизводительную и отказоустойчивую конвертацию **больших** Excel-файлов (формат `.xlsx`, спецификация OPC ZIP) в один из выбранных структурированных форматов: CSV (чанками), NDJSON или JSON-массив.

**Место в системе CATMEPIM:** Данный модуль является **ключевым компонентом этапа извлечения (Extract)** в общем ETL-процессе (см. `ETL-Process-Contract.md`). Он принимает на вход исходные Excel-файлы (предположительно, после этапа определения поставщика `VendorDetector`, который находится вне рамок ответственности этого модуля) и преобразует их в стандартизированный потоковый формат (NDJSON предпочтителен для последующих шагов), который затем передается на этап преобразования (Transform), обычно начинающийся с модуля `Deduplicator`. Этот конвертер предназначен для решения проблемы производительности и потребления памяти существующего конвертера при работе с файлами размером > 300 МБ.

**Ключевые особенности:**

1.  **Автоматический выбор стратегии:**
    *   Файлы ≤ 300 МБ: Используется оптимизированный SAX-парсер на базе библиотеки `EasyExcel` (обертка над POI-SAX).
    *   Файлы > 300 МБ: Используется собственная реализация потоковой распаковки (`java.util.zip`) и SAX-парсинга XML-компонентов (`sharedStrings.xml`, `sheetN.xml`).
2.  **Отказоустойчивость:** Гарантированная обработка файлов с аномально высоким коэффициентом сжатия (zip-бомбы) без исчерпания памяти (OOM Error) за счет потоковой обработки и фоллбэк-механизмов.
3.  **Производительность:** Оптимизирован для минимизации использования RAM и CPU на больших объемах данных через потоковые API, ленивую загрузку и батчинг.

---

## 2. Границы ответственности

### Входит в зону ответственности

*   Чтение и распаковка `.xlsx` файлов (OPC-ZIP формат) с использованием `POI-OpenXML4J` (с безопасными настройками `ZipSecureFile.setMinInflateRatio(0.01)` и `ZipSecureFile.setMaxEntrySize(6_442_450_944L)` (6GB), подобранными для обработки файлов >700MB) и ручным фоллбэком через `java.util.zip` при обнаружении zip-бомб, если стандартные настройки POI не справляются.
*   Потоковый разбор словаря строк (`sharedStrings.xml`) с использованием ленивой загрузки и, возможно, сегментированного LRU-кэша для минимизации потребления памяти.
*   Потоковый SAX-парсинг данных листа (`worksheets/sheetN.xml`) с использованием кастомного `SheetHandler`.
*   Преобразование строк Excel в объекты (`Map<String, Object>` или типизированные DTO, если возможно) с использованием конфигурируемого `RowMapper`.
*   Буферизация обработанных записей в батчи заданного размера (`batchSize`).
*   Запись батчей данных в указанный выходной поток/файл в выбранном формате (CSV чанками, NDJSON, JSON-массив) через абстракцию `DataWriter`.
*   Автоматическое переключение между стратегиями обработки (`EasyExcel` vs Streaming) на основе размера входного файла.
*   Сбор и логирование базовых метрик процесса (количество строк, объем данных, время выполнения этапов, использованная стратегия).

### Не входит в зону ответственности

*   **Определение поставщика (Vendor Detection):** Предполагается, что это выполнено на предыдущем шаге ETL.
*   **Дедупликация данных:** Выполняется модулем `Deduplicator`.
*   **Нормализация и очистка данных:** Выполняется последующими модулями (например, `JsonCleaner` или в рамках `Deduplicator`/`DatabaseLoader`).
*   **Обогащение данных:** Выполняется модулем `DataEnricher`.
*   **Валидация данных** по бизнес-правилам или JSON-Schema.
*   **Загрузка данных** в базу данных: Выполняется модулем `DatabaseLoader`.
*   **Обработка старого формата `.xls` (BIFF):** Поддерживается только современный формат `.xlsx` (OOXML).
*   **Обработка нескольких листов одновременно:** Обрабатывается только один указанный лист (по умолчанию первый видимый или `sheet1.xml`).
*   **Обработка защищенных паролем файлов.**

---

## 3. Предусловия (Preconditions)

*   **Входной файл:**
    *   Путь к входному файлу (`.xlsx`) указан и файл существует.
    *   Файл доступен для чтения процессом.
    *   Файл имеет *минимально необходимую* структуру формата XLSX (OPC ZIP), позволяющую идентифицировать компоненты (наличие `sharedStrings.xml`, `workbook.xml`, `worksheets/sheetN.xml`). (Более строгая проверка на корректность XML происходит во время парсинга).
*   **Конфигурация:**
    *   Параметры конфигурации (через CLI или файл) корректны:
        *   `--format` указан и соответствует одному из поддерживаемых (`csv`, `ndjson`, `json`).
        *   `--batchSize` является целым числом > 0.
        *   `--sheetName` (если указан) синтаксически корректен. (Проверка на существование листа происходит позже).
    *   Целевой путь/поток для вывода указан и доступен для записи. Директория `data/temp/` (относительно корня проекта) доступна для записи временных файлов и CSV-чанков.
*   **Окружение:**
    *   Система запущена в среде Java 17+.
    *   Доступно достаточное количество RAM (минимум 2 GB *доступной Heap-памяти*, см. п.11) для работы POI и буферов.
    *   Доступно достаточное дисковое пространство и производительность I/O (SSD рекомендуется) для временных файлов и записи выходных данных.

---

## 4. Постусловия (Postconditions)

*   **При успешном завершении:**
    *   Входной `.xlsx` файл остается неизменным.
    *   В указанном месте создан выходной файл (или данные записаны в поток) в выбранном формате (`--format`).
    *   Содержимое выходного файла корректно отражает данные из указанного листа входного `.xlsx` файла, преобразованные через `RowMapper`.
    *   **Гарантируется сохранение порядка строк** в выходном файле относительно их порядка в исходном листе Excel.
    *   Все строки из указанного листа были обработаны (или пропущены с логированием, если `continueOnError=true` и ошибка была на уровне структуры строки/XML, а не типа данных).
    *   **Для CSV:** В директории `data/temp/` создан один или несколько файлов с именами вида `<original_filename>-chunk-N.csv` (где `<original_filename>` - имя входного файла без расширения, N - номер чанка, начиная с 0). Каждый чанк (кроме, возможно, последнего) содержит не более `batchSize` строк данных.
    *   **Для NDJSON:** Создан один файл `output.ndjson` (путь указан в `--output`), где каждая строка является валидным JSON-объектом.
    *   **Для JSON:** Создан один файл `output.json` (путь указан в `--output`), содержащий валидный JSON-массив объектов.
    *   Процесс завершен без необработанных исключений.
    *   В логах доступна информация об использованной стратегии, итоговые метрики (общее кол-во строк, байт прочитано/записано, время выполнения) и предупреждения (если были).
*   **При возникновении неустранимой ошибки:**
    *   Процесс завершается с ошибкой.
    *   В логах зафиксирована информация о причине сбоя.
    *   Частично созданные выходные файлы (включая CSV-чанки в `data/temp/`) могут остаться, но их консистентность не гарантируется (кроме уже полностью записанных CSV-чанков).
    *   Система находится в стабильном состоянии (ресурсы, такие как файловые дескрипторы, освобождены).

---

## 5. Инварианты (Invariants)

*   **Потоковая обработка (Streaming Invariant):** Вне зависимости от размера входного файла, модуль не загружает весь файл или его крупные части (как `sharedStrings.xml` или `sheetN.xml`) целиком в оперативную память. Потребление памяти должно быть ограничено размером текущего обрабатываемого блока XML, размером буфера записей (`batchSize`) и размером кэша строк (для `LazySharedStringsProvider`).
*   **Целостность данных и порядка строк (Data & Order Integrity):** Преобразование данных из строки Excel в выходной объект (`Map<String, Object>`) выполняется единообразно для всех строк с помощью одного и того же экземпляра `RowMapper` и актуального словаря строк. Порядок обработки и вывода строк соответствует их порядку в исходном листе.
*   **Состояние батча (Batch State):** Буфер записей (`BatchBuffer`) накапливает не более `batchSize` элементов перед принудительной записью (`flush`) через `DataWriter`.
*   **Неизменность конфигурации (Configuration Immutability):** Начальная конфигурация (`ConverterConfig`) остается неизменной на протяжении всего процесса конвертации.
*   **Отказоустойчивость к Zip Bomb (Zip Bomb Resilience):** При работе со `UserModeEventConversionStrategy` модуль корректно обрабатывает файлы с аномально низким коэффициентом сжатия без возникновения OOM Error, в первую очередь за счет адекватной настройки `ZipSecureFile.setMaxEntrySize()` (текущее значение 6GB успешно для файлов ~700MB). Переключение на ручную распаковку (`FallbackZipExtractor`) является запасным механизмом.
*   **Производительность `LazySharedStringsProvider`:** Эффективность `UserModeEventConversionStrategy` зависит от производительности `LazySharedStringsProvider`. Если для кэширования строк используется диск, общая производительность может стать зависимой от скорости I/O подсистемы (это осознанный компромисс - "gas price" за обработку больших словарей строк в ограниченной RAM).

---

## 6. Интерфейсы (Входы и Выходы)

### Входы

*   **Источник данных:** Путь к входному файлу формата `.xlsx`.
*   **Параметры конфигурации:**
    *   Через аргументы командной строки (CLI) или конфигурационный файл (`application.yml`).
    *   Обязательные:
        *   `--input`: Путь к входному `.xlsx` файлу.
        *   `--output`:
            *   Для `ndjson`, `json`: Путь к целевому *файлу*.
            *   Для `csv`: **Параметр игнорируется.** Выходные файлы всегда записываются в директорию `data/temp/` (см. раздел Выходы).
        *   `--format`: Целевой формат (`csv`, `ndjson`, `json`).
    *   Опциональные:
        *   `--batchSize`: Размер пакета записей для внутреннего батчинга и частоты логирования (по умолчанию: 50 000).
        *   `--sheetName`: Имя листа для обработки. **Если не указано, используется первый лист в порядке, предоставляемом `org.apache.poi.xssf.eventusermodel.XSSFReader.SheetIterator` (обычно соответствует порядку в `workbook.xml`).** Если лист с указанным именем не найден, генерируется ошибка инициализации.
        *   `--continueOnError`: Флаг продолжения обработки при ошибках парсинга структуры строк/XML (по умолчанию: `false` - fail fast). **Ошибки несоответствия типов данных в ячейках этим флагом не обрабатываются** (см. п.9).
        *   Параметры для `RowMapper` (например, маппинг колонок).
        *   Параметры для `DataWriter` (например, разделитель для CSV).

### Выходы

*   **Результат конвертации:**
    *   При `--format=csv`: Один или несколько файлов `<original_filename>-chunk-N.csv` в директории `data/temp/`.
    *   При `--format=ndjson`: Один файл, указанный в `--output`.
    *   При `--format=json`: Один файл, указанный в `--output`.
*   **Логирование:** Запись информации о ходе процесса, ошибках, предупреждениях и итоговых метриках в стандартный вывод или настроенный файл логов.
*   **Метрики:** Сбор и предоставление (через логи или отдельный механизм) числовых показателей процесса (детали см. в разделе 10).

---

## 7. Архитектурные компоненты

```text
HighVolumeExcelConverter (Core Orchestrator)
├─ ConverterConfig             # Парсер конфигурации (CLI/YAML)
├─ StrategySelector            # Выбирает EasyExcel или UserModeEventConversionStrategy по размеру файла
├─ EasyExcelStrategy           # Реализация через EasyExcel (для < 300MB)
│  └─ EasyExcelRowListener      # Адаптер для EasyExcel, вызывает RowMapper+BatchBuffer
└─ UserModeEventConversionStrategy # Реализация через java.util.zip + SAX (для > 300MB)
   ├─ SafePOIEntryStreamer     # Безопасный стример для XML-частей XLSX (из POI)
   ├─ FallbackZipExtractor     # Ручной ZIP-стример (фоллбэк для ZipBomb)
   ├─ LazySharedStringsProvider# Ленивый/сегментированный SAX-парсер/кэш для sharedStrings.xml
   ├─ SheetXmlHandler          # SAX-handler для worksheets/sheetN.xml, использует LazySharedStringsProvider
   ├─ RowMapper<Map<String,Object>> # Преобразует XML-строку в Map/Object. **Не выполняет сложную валидацию/преобразование типов данных** (ответственность последующих этапов ETL). Извлекает значения ячеек как есть (или как базовые типы POI).
   ├─ BatchBuffer<T>           # Буфер для накопления batchSize записей
   ├─ DataWriter<T>            # Абстрактный писатель батчей
   │  ├─ CsvBatchWriter        # Реализация для CSV (использует Commons-CSV)
   │  ├─ NdjsonWriter          # Реализация для NDJSON (Jackson line-by-line)
   │  └─ JsonArrayWriter       # Реализация для JSON-массива (Jackson Streaming API)
   └─ MetricsCollector         # Сборщик метрик (агрегирует по батчам/этапам)
```

> **Примечание по реализации:**
> - Для `JsonArrayWriter` и `NdjsonWriter` рекомендуется использовать Jackson\'s JsonGenerator для эффективной потоковой записи.
> - Все реализации `DataWriter` должны использовать `BufferedOutputStream` с размером буфера 128КB для оптимизации I/O.
> - Оптимальный размер батча для записи (`batchSize`) составляет ~50,000 строк, что балансирует потребление памяти и накладные расходы на I/O.

---

## 8. Алгоритм работы

1.  **Инициализация:**
    *   `ZipSecureFile.setMinInflateRatio(0.01);` // Модуль сам устанавливает настройку безопасности POI.
    *   Парсинг конфигурации (`ConverterConfig.fromArgs(args)` или из файла).
    *   Определение целевого листа для обработки. Если имя или индекс листа не указаны в конфигурации, выбирается первый доступный лист. Для этого используется `org.apache.poi.xssf.eventusermodel.XSSFReader.SheetIterator` без загрузки всей структуры книги (`XSSFWorkbook`) в память. При ошибке (например, лист не найден) - завершение.
    *   Создание `MetricsCollector`.
    *   Создание фабрикой нужного `DataWriter` (`WriterFactory.create(...)`) на основе `--format`.
    *   Создание `RowMapper`.
    *   Создание `BatchBuffer`, связанного с `DataWriter` и `MetricsCollector`.
2.  **Выбор стратегии:** Определение размера входного файла и выбор соответствующей стратегии (`EasyExcelStrategy` или `UserModeEventConversionStrategy`). Логирование выбранной стратегии.
3.  **Выполнение стратегии (`strategy.execute()`):** Запуск основного процесса конвертации согласно выбранной стратегии (либо через `EasyExcel`, либо через ручной SAX-парсинг `sharedStrings.xml` и `sheetN.xml`). `RowMapper` используется для преобразования строк, `BatchBuffer` для накопления.
4.  **Запись данных:** `BatchBuffer` периодически (по достижении `batchSize` или при завершении) вызывает `DataWriter` для записи накопленных данных в выходной файл/поток.
5.  **Завершение:** Вызов `batchBuffer.flush()` для записи остатков, `dataWriter.close()` для финализации вывода, освобождение ресурсов и логирование итоговых метрик.

---

## 9. Обработка ошибок и фоллбэки

*   **ZipBombDetectedException (или аналог от POI):** Автоматический переход от `SafePOIEntryStreamer` к `FallbackZipExtractor` для `sharedStrings.xml` и `sheetN.xml` в рамках `UserModeEventConversionStrategy`. Логируется предупреждение.
*   **Ошибка парсинга структуры строки Excel/XML:** Если `continueOnError=true`, ошибка логируется, счетчик `excel.rows.skipped` инкрементируется, строка пропускается. Если `false`, выбрасывается исключение (fail fast).
*   **Ошибка несоответствия типа данных в ячейке:** **Не считается ошибкой на уровне конвертера.** `RowMapper` извлекает данные как есть (например, как строку или базовый тип POI). Валидация и преобразование типов - задача последующих этапов ETL. Если POI не может прочитать ячейку, это может трактоваться как структурная ошибка строки (см. предыдущий пункт).
*   **Ошибка записи батча (`DataWriter.writeItems`):** Логируется ошибка. Возможна одна попытка повторной записи (retry). При повторной неудаче процесс аварийно завершается с записью ошибки в лог.
*   **Некорректный формат файла / Отсутствие листа / Поврежденный файл:** Ошибка на этапе инициализации или во время парсинга. Процесс завершается с информативным сообщением в логе.
*   **I/O ошибки (чтение/запись):** Логируются, процесс аварийно завершается.

---

## 10. Метрики

*   **Общие:**
    *   `excel.converter.strategy`: `easy_excel` | `streaming`
    *   `excel.rows.processed`: Общее число успешно обработанных строк.
    *   `excel.rows.skipped`: Общее число строк, пропущенных из-за ошибок (при `continueOnError=true`).
    *   `excel.bytes.read`: Общий объем прочитанных байт из входного файла (приблизительно).
    *   `excel.bytes.written`: Общий объем записанных байт в выходной файл(ы).
    *   `excel.batches.written`: Общее количество записанных батчей.
*   **Время выполнения (мс):**
    *   `excel.time.total`: Общее время выполнения конвертации.
    *   `excel.time.stage.init`: Время инициализации.
    *   `excel.time.stage.easy_excel`: Время работы `EasyExcel` (если применимо).
    *   `excel.time.stage.shared_strings`: Время парсинга `sharedStrings.xml` (для `UserModeEventConversionStrategy`).
    *   `excel.time.stage.sheet_parsing`: Время парсинга `sheetN.xml` (для `UserModeEventConversionStrategy`).
    *   `excel.time.stage.writing`: Суммарное время, затраченное на операции записи (`DataWriter.writeItems`).

---

## 11. Ограничения и требования

*   **Формат:** Поддерживается только формат `.xlsx` (OOXML на базе OPC ZIP). Формат `.xls` (BIFF) **не поддерживается**.
*   **Листы:** Обрабатывается только один лист, указанный в `--sheetName` (или первый видимый по умолчанию).
*   **Защита:** Защищенные паролем файлы **не поддерживаются**.
*   **Среда:** Требуется **Java 17+**.
*   **Ресурсы:**
    *   **RAM:** Требуется **минимум 2 GB *свободной Heap-памяти* (`-Xmx2g`)** для Java-процесса. **Рекомендуется 4GB+ (`-Xmx4g`)** для стабильной обработки файлов > 1 GB. Для файлов ~700MB с очень большими внутренними XML-компонентами (например, `sheet1.xml` > 5GB в распакованном виде) требуется не только достаточный Heap (`-Xmx4g` или выше), но и соответствующая настройка `ZipSecureFile.setMaxEntrySize()` (например, 6GB), чтобы избежать `ZipBombDetectedException`.
    *   **JVM:** Рекомендуется использовать сборщик мусора G1GC (`-XX:+UseG1GC`) с дополнительными настройками:
        * `-XX:MaxGCPauseMillis=200` для ограничения пауз сборки мусора
        * `-XX:ConcGCThreads=2` для оптимизации параллельной сборки
        * `-Xms1g -Xmx4g` для стабильного размера хипа
    *   **I/O:** Требуется достаточно быстрый диск (SSD рекомендуется) для эффективной работы с потоками и временными файлами (если кэш строк использует диск).
*   **Восстановление после сбоя:** В данной версии (2.0.1) **не поддерживается механизм чекпоинтов и автоматического возобновления** процесса с места сбоя. Однако, при использовании формата вывода `csv`, уже успешно записанные файлы-чанки (`*-chunk-N.csv`) остаются в `data/temp/`, что обеспечивает возможность частичного ручного или полуавтоматического восстановления обработки.

---

## 12. Примеры использования

**Командная строка (CLI):**

```bash
# Конвертация в NDJSON с размером батча по умолчанию
java -jar high-volume-converter.jar \
  --input=/path/to/large-vendor-data.xlsx \
  --output=/path/to/output/data.ndjson \
  --format=ndjson

# Конвертация в CSV чанками по 50000 строк, пропуская структурные ошибки
# Чанки будут сохранены в data/temp/large-vendor-data-chunk-N.csv
java -jar high-volume-converter.jar \
  --input=/path/to/large-vendor-data.xlsx \
  # --output параметр игнорируется для формата csv, т.к. используется data/temp/
  --format=csv \
  --batchSize=50000 \
  --continueOnError=true
```

**Сервисный режим (через конфигурационный файл):**

```bash
# Запуск с указанием пути к файлу конфигурации
java -Dconfig.path=/etc/catmepim/converter.yml -jar high-volume-converter.jar
```
*Содержимое `converter.yml`:*

```yaml
input: /mnt/data/incoming/another_large_file.xlsx
output: /mnt/data/processed/output_data.json
format: json
batchSize: 10000
# sheetName: Sheet1 # опционально
# continueOnError: false # опционально
```

---

## 13. Связанные контракты

*   **System-Contract.md:** Общий контракт системы CATMEPIM.
*   **ETL-Process-Contract.md:** Описывает место конвертера в общем ETL-потоке.
*   **Deduplicator-Architecture-Contract.md:** Контракт модуля, который обычно является потребителем данных от этого конвертера.
*   **DatabaseLoader-Architecture-Contract.md:** Контракт модуля на последнем этапе ETL.
*   *(Опционально) VendorDetector Contract:* Контракт модуля, который может предшествовать этому конвертеру в ETL-цепочке.

---

*Этот контракт определяет требования к новой, высокопроизводительной реализации конвертера Excel-файлов для CATMEPIM, способной обрабатывать большие объемы данных, и готов к рассмотрению и утверждению командой разработки.*

---

## 14. Явные контракты в коде (Explicit Contracts in Code)

Все ключевые классы и публичные методы, реализующие данный контракт, должны содержать в Javadoc или явных комментариях:
- @pre (предусловия) — что должно быть истинно до вызова метода/конструктора;
- @post (постусловия) — что гарантируется после успешного завершения метода;
- @invariant (инварианты) — что должно оставаться истинным на протяжении жизни объекта/процесса;
- @throws — какие исключения могут быть выброшены при нарушении предусловий или ошибках выполнения;
- Ссылку на актуальную версию этого контракта (HighVolumeExcelConverter-Contract-v2.0.1.md).

**Проверки предусловий должны быть реализованы явно (например, через IllegalArgumentException).**
**Постусловия и инварианты должны быть либо явно проверяемы, либо документированы и поддерживаться логикой.** 